{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Brief primer and history\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an [orthogonal transformation](https://en.wikipedia.org/wiki/Orthogonal_transformation) to convert a set of observations of possibly correlated variables into a set of values of [linearly uncorrelated](https://en.wikipedia.org/wiki/Correlation_and_dependence) variables called principal components. The number of distinct principal components is equal to the smaller of the number of original variables or the number of observations minus one. This transformation is defined in such a way that the first principal component has the largest possible [variance](https://en.wikipedia.org/wiki/Variance) (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is [orthogonal](https://en.wikipedia.org/wiki/Orthogonal) the preceding components. The resulting vectors are an uncorrelated [orthogonal basis set](https://en.wikipedia.org/wiki/Orthogonal_basis_set). \n",
    "\n",
    "PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "PCA was invented in 1901 by [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by [Harold Hotelling](https://en.wikipedia.org/wiki/Harold_Hotelling) in the 1930s.\n",
    "\n",
    "## Mathematical details\n",
    "PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.[3]\n",
    "\n",
    "Consider a data matrix, $\\mathbf{X}$, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the $n$ rows represents a different repetition of the experiment, and each of the $p$ columns gives a particular kind of feature (say, the results from a particular sensor).\n",
    "\n",
    "Mathematically, the transformation is defined by a set of p-dimensional vectors of weights or loadings\n",
    "${\\displaystyle \\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}} \\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}$ that map each row vector ${\\displaystyle \\mathbf {x} _{(i)}} \\mathbf{x}_{(i)}$ of $\\mathbf{X}$ to a new vector of principal component scores ${\\displaystyle \\mathbf {t} _{(i)}=(t_{1},\\dots ,t_{m})_{(i)}}$ given by\n",
    "\n",
    "$${\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,m} {\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,m}$$\n",
    "\n",
    "in such a way that the individual variables ${\\displaystyle t_{1},\\dots ,t_{m}}$ of t considered over the data set successively inherit the maximum possible variance from $\\mathbf{x}$, with each loading vector $\\mathbf{w}$ constrained to be a unit vector.\n",
    "\n",
    "In order to maximize variance, the first loading vector $\\mathbf {w} _{(1)}$ thus has to satisfy\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {w} _{(1)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\sum _{i}\\left(t_{1}\\right)_{(i)}^{2}\\right\\}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\sum _{i}\\left(\\mathbf {x} _{(i)}\\cdot \\mathbf {w} \\right)^{2}\\right\\}}$$\n",
    "\n",
    "Equivalently, writing this in matrix form gives\n",
    "\n",
    "$${\\displaystyle \\mathbf {w} _{(1)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\{\\Vert \\mathbf {Xw} \\Vert ^{2}\\}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\mathbf {w} ^{T}\\mathbf {X} ^{T}\\mathbf {Xw} \\right\\}}$$\n",
    "\n",
    "Since $${\\mathbf {w} _{(1)}}$$ has been defined to be a unit vector, it equivalently also satisfies\n",
    "${\\displaystyle \\mathbf {w} _{(1)}=\\,\\left\\{{\\frac {\\mathbf {w} ^{T}\\mathbf {X} ^{T}\\mathbf {Xw} }{\\mathbf {w} ^{T}\\mathbf {w} }}\\right\\}}$.\n",
    "\n",
    "With $\\mathbf {w} _{(1)}$ found, the first principal component of a data vector $\\mathbf {x} _{(i)}$ can then be given as a score $\\mathbf {t} _{(i)}$ = $\\mathbf {x} _{(i)}$ ⋅ $\\mathbf {w} _{(1)}$ in the transformed co-ordinates, or as the corresponding vector in the original variables, {$\\mathbf {x} _{(i)}$ ⋅ $\\mathbf {w} _{(1)}$} $\\mathbf {w} _{(1)}$.\n",
    "\n",
    "The $k^{th}$ component can be found by subtracting the first $k$ − 1 principal components from $\\mathbf{X}$:\n",
    "\n",
    "$${\\displaystyle \\mathbf {\\hat {X}} _{k}=\\mathbf {X} -\\sum _{s=1}^{k-1}\\mathbf {X} \\mathbf {w} _{(s)}\\mathbf {w} _{(s)}^{\\rm {T}}}$$\n",
    "and then finding the loading vector which extracts the maximum variance from this new data matrix\n",
    "\n",
    "$${\\displaystyle \\mathbf {w} _{(k)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {arg\\,max} }}\\left\\{\\Vert \\mathbf {\\hat {X}} _{k}\\mathbf {w} \\Vert ^{2}\\right\\}={\\operatorname {\\arg \\,max} }\\,\\left\\{{\\tfrac {\\mathbf {w} ^{T}\\mathbf {\\hat {X}} _{k}^{T}\\mathbf {\\hat {X}} _{k}\\mathbf {w} }{\\mathbf {w} ^{T}\\mathbf {w} }}\\right\\}}$$\n",
    "\n",
    "Computing the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) is now the standard way to calculate a principal components analysis from a data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and perform basic exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine.data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots by output labels/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c in df.columns[1:]:\n",
    "    df.boxplot(c,by='Class',figsize=(5,3),fontsize=14)\n",
    "    plt.title(\"{}\\n\".format(c),fontsize=16)\n",
    "    plt.xlabel(\"Wine Class\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It can be seen that some features classify the wine labels pretty clearly.** For example, Alcalinity, Total Phenols, or Flavonoids produce boxplots with well-separated medians, which are clearly indicative of wine classes.\n",
    "\n",
    "Below is an example of class seperation using two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df['OD280/OD315 of diluted wines'],df['Flavanoids'],c=df['Class'],edgecolors='k',alpha=0.75,s=50)\n",
    "plt.grid(True)\n",
    "plt.title(\"Scatter plot of two features showing the \\ncorrelation and class seperation\",fontsize=15)\n",
    "plt.xlabel(\"OD280/OD315 of diluted wines\",fontsize=15)\n",
    "plt.ylabel(\"Flavanoids\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the features independent? Plot co-variance matrix\n",
    "\n",
    "It can be seen that there are some good amount of correlation between features i.e. they are not independent of each other, as assumed in Naive Bayes technique. However, we will still go ahead and apply the classifier to see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "    from matplotlib import colormaps\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = colormaps.get_cmap('jet')\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title('Wine data set features correlation\\n',fontsize=15)\n",
    "    labels=df.columns\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    fig.colorbar(cax, ticks=[0.1*i for i in range(-11,11)])\n",
    "    ax1.tick_params()\n",
    "    ax1.set_xticks(ticks=list(range(0,14)))\n",
    "    ax1.set_yticks(ticks=list(range(0,14)))\n",
    "    ax1.set_xticklabels(labels,fontsize=5)\n",
    "    ax1.set_yticklabels(labels,fontsize=8)\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling\n",
    "PCA requires scaling/normalization of the data to work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop('Class',axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.DataFrame(data=X,columns=df.columns[1:])\n",
    "dfx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA class import and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx_pca = pca.fit(dfx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the _explained variance ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=[i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],\n",
    "            y=dfx_pca.explained_variance_ratio_,\n",
    "            s=200, alpha=0.75,c='orange',edgecolor='k')\n",
    "plt.grid(True)\n",
    "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=15)\n",
    "plt.xlabel(\"Principal components\",fontsize=15)\n",
    "plt.xticks([i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylabel(\"Explained variance ratio\",fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above plot means that the $1^{st}$ principal component explains about 36% of the total variance in the data and the $2^{nd}$ component explians further 20%. Therefore, if we just consider first two components, they together explain 56% of the total variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing better class separation using principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the scaled data set using the fitted PCA object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx_trans = pca.transform(dfx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx_trans = pd.DataFrame(data=dfx_trans)\n",
    "dfx_trans.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the first two columns of this transformed data set with the color set to original ground truth class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(dfx_trans[0],dfx_trans[1],c=df['Class'],edgecolors='k',alpha=0.75,s=150)\n",
    "plt.grid(True)\n",
    "plt.title(\"Class separation using first two principal components\\n\",fontsize=20)\n",
    "plt.xlabel(\"Principal component-1\",fontsize=15)\n",
    "plt.ylabel(\"Principal component-2\",fontsize=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
